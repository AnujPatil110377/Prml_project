{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-19T01:37:34.343690Z","iopub.status.busy":"2024-04-19T01:37:34.343230Z","iopub.status.idle":"2024-04-19T01:37:34.947398Z","shell.execute_reply":"2024-04-19T01:37:34.945434Z","shell.execute_reply.started":"2024-04-19T01:37:34.343658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["       textID                                               text  \\\n","0  cb774db0d1                I`d have responded, if I were going   \n","1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n","2  088c60f138                          my boss is bullying me...   \n","3  9642c003ef                     what interview! leave me alone   \n","4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n","\n","                         selected_text sentiment Time of Tweet Age of User  \\\n","0  I`d have responded, if I were going   neutral       morning        0-20   \n","1                             Sooo SAD  negative          noon       21-30   \n","2                          bullying me  negative         night       31-45   \n","3                       leave me alone  negative       morning       46-60   \n","4                        Sons of ****,  negative          noon       60-70   \n","\n","       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n","0  Afghanistan        38928346.0         652860.0             60.0  \n","1      Albania         2877797.0          27400.0            105.0  \n","2      Algeria        43851044.0        2381740.0             18.0  \n","3      Andorra           77265.0            470.0            164.0  \n","4       Angola        32866272.0        1246700.0             26.0  \n"]}],"source":["import pandas as pd\n","\n","df1 = pd.read_csv('train.csv',encoding='latin1')\n","df2 = pd.read_csv('test.csv',encoding='latin1')\n","\n","# Merge the DataFrames\n","train_data = pd.concat([df1, df2], ignore_index=True)\n","\n","# Write the merged DataFrame to a new CSV file\n","train_data.to_csv('merged_file.csv', index=False)\n","print(train_data.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["Removing the Unnecessary columns"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:37:39.371712Z","iopub.status.busy":"2024-04-19T01:37:39.370463Z","iopub.status.idle":"2024-04-19T01:37:39.382078Z","shell.execute_reply":"2024-04-19T01:37:39.380404Z","shell.execute_reply.started":"2024-04-19T01:37:39.371677Z"},"trusted":true},"outputs":[],"source":["columns_to_remove = ['textID', 'selected_text', 'Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)']\n","train_data.drop(columns=columns_to_remove, inplace=True)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:37:42.065849Z","iopub.status.busy":"2024-04-19T01:37:42.065069Z","iopub.status.idle":"2024-04-19T01:37:42.080171Z","shell.execute_reply":"2024-04-19T01:37:42.079026Z","shell.execute_reply.started":"2024-04-19T01:37:42.065803Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my boss is bullying me...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>what interview! leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>32291</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32292</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32293</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32294</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32295</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>32296 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    text sentiment\n","0                    I`d have responded, if I were going   neutral\n","1          Sooo SAD I will miss you here in San Diego!!!  negative\n","2                              my boss is bullying me...  negative\n","3                         what interview! leave me alone  negative\n","4       Sons of ****, why couldn`t they put them on t...  negative\n","...                                                  ...       ...\n","32291                                                NaN       NaN\n","32292                                                NaN       NaN\n","32293                                                NaN       NaN\n","32294                                                NaN       NaN\n","32295                                                NaN       NaN\n","\n","[32296 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:37:45.393561Z","iopub.status.busy":"2024-04-19T01:37:45.393032Z","iopub.status.idle":"2024-04-19T01:37:45.434840Z","shell.execute_reply":"2024-04-19T01:37:45.433152Z","shell.execute_reply.started":"2024-04-19T01:37:45.393509Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing Values:\n"," text         1282\n","sentiment    1281\n","dtype: int64\n","\n","Duplicate Rows: 1280\n"]}],"source":["# Check for missing values\n","missing_values = train_data.isnull().sum()\n","print(\"Missing Values:\\n\", missing_values)\n","\n","# Check for duplicates\n","duplicate_rows = train_data.duplicated().sum()\n","print(\"\\nDuplicate Rows:\", duplicate_rows)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:37:48.121570Z","iopub.status.busy":"2024-04-19T01:37:48.121153Z","iopub.status.idle":"2024-04-19T01:37:48.137707Z","shell.execute_reply":"2024-04-19T01:37:48.135593Z","shell.execute_reply.started":"2024-04-19T01:37:48.121540Z"},"trusted":true},"outputs":[],"source":["train_data.dropna(subset=['text'], inplace=True)\n","# train_data.dropna(subset=['text_lower'],inplace=True)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:37:50.965382Z","iopub.status.busy":"2024-04-19T01:37:50.964966Z","iopub.status.idle":"2024-04-19T01:37:50.983116Z","shell.execute_reply":"2024-04-19T01:37:50.981600Z","shell.execute_reply.started":"2024-04-19T01:37:50.965355Z"},"trusted":true},"outputs":[],"source":["# Lowercase Conversion\n","train_data['text_lower'] = train_data['text'].str.lower()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:38:01.703114Z","iopub.status.busy":"2024-04-19T01:38:01.702690Z","iopub.status.idle":"2024-04-19T01:38:01.718499Z","shell.execute_reply":"2024-04-19T01:38:01.717036Z","shell.execute_reply.started":"2024-04-19T01:38:01.703084Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>text_lower</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","      <td>i`d have responded, if i were going</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>negative</td>\n","      <td>sooo sad i will miss you here in san diego!!!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my boss is bullying me...</td>\n","      <td>negative</td>\n","      <td>my boss is bullying me...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>what interview! leave me alone</td>\n","      <td>negative</td>\n","      <td>what interview! leave me alone</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>negative</td>\n","      <td>sons of ****, why couldn`t they put them on t...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31010</th>\n","      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n","      <td>negative</td>\n","      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n","    </tr>\n","    <tr>\n","      <th>31011</th>\n","      <td>All alone in this old house again.  Thanks for...</td>\n","      <td>positive</td>\n","      <td>all alone in this old house again.  thanks for...</td>\n","    </tr>\n","    <tr>\n","      <th>31012</th>\n","      <td>I know what you mean. My little dog is sinkin...</td>\n","      <td>negative</td>\n","      <td>i know what you mean. my little dog is sinkin...</td>\n","    </tr>\n","    <tr>\n","      <th>31013</th>\n","      <td>_sutra what is your next youtube video gonna b...</td>\n","      <td>positive</td>\n","      <td>_sutra what is your next youtube video gonna b...</td>\n","    </tr>\n","    <tr>\n","      <th>31014</th>\n","      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n","      <td>positive</td>\n","      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31014 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                    text sentiment  \\\n","0                    I`d have responded, if I were going   neutral   \n","1          Sooo SAD I will miss you here in San Diego!!!  negative   \n","2                              my boss is bullying me...  negative   \n","3                         what interview! leave me alone  negative   \n","4       Sons of ****, why couldn`t they put them on t...  negative   \n","...                                                  ...       ...   \n","31010  its at 3 am, im very tired but i can`t sleep  ...  negative   \n","31011  All alone in this old house again.  Thanks for...  positive   \n","31012   I know what you mean. My little dog is sinkin...  negative   \n","31013  _sutra what is your next youtube video gonna b...  positive   \n","31014   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n","\n","                                              text_lower  \n","0                    i`d have responded, if i were going  \n","1          sooo sad i will miss you here in san diego!!!  \n","2                              my boss is bullying me...  \n","3                         what interview! leave me alone  \n","4       sons of ****, why couldn`t they put them on t...  \n","...                                                  ...  \n","31010  its at 3 am, im very tired but i can`t sleep  ...  \n","31011  all alone in this old house again.  thanks for...  \n","31012   i know what you mean. my little dog is sinkin...  \n","31013  _sutra what is your next youtube video gonna b...  \n","31014   http://twitpic.com/4woj2 - omgssh  ang cute n...  \n","\n","[31014 rows x 3 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"markdown","metadata":{},"source":["*****Text cleaning***"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:38:08.468825Z","iopub.status.busy":"2024-04-19T01:38:08.468377Z","iopub.status.idle":"2024-04-19T01:38:08.794800Z","shell.execute_reply":"2024-04-19T01:38:08.793373Z","shell.execute_reply.started":"2024-04-19T01:38:08.468796Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def clean_text(text):\n","    if isinstance(text, str):  # Check if text is a string\n","        # Remove special characters, HTML tags, and links\n","        cleaned_text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n","        cleaned_text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", cleaned_text)  # Remove links\n","        cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned_text)  # Remove special characters\n","        return cleaned_text.lower()  # Convert text to lowercase\n","    else:\n","        return text  # Return unchanged if not a string\n","\n","# Apply text cleaning to 'text' column\n","train_data['text'] = train_data['text'].apply(clean_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Tokenization: Split the text into individual words or tokens for further analysis.**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:38:15.427901Z","iopub.status.busy":"2024-04-19T01:38:15.427489Z","iopub.status.idle":"2024-04-19T01:38:15.513084Z","shell.execute_reply":"2024-04-19T01:38:15.511920Z","shell.execute_reply.started":"2024-04-19T01:38:15.427873Z"},"trusted":true},"outputs":[],"source":["def tokenize_text(text):\n","    if isinstance(text, str):\n","        # Split the text into tokens using whitespace as the delimiter\n","        tokens = text.split()\n","        return tokens\n","    else:\n","        return []\n","\n","# Applying the tokenization function to the 'text' column in the train_data DataFrame\n","train_data['tokens'] = train_data['text'].apply(tokenize_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Stopwords Removal: Remove common stopwords while preserving the links.**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:38:22.060730Z","iopub.status.busy":"2024-04-19T01:38:22.060261Z","iopub.status.idle":"2024-04-19T01:38:27.306346Z","shell.execute_reply":"2024-04-19T01:38:27.304969Z","shell.execute_reply.started":"2024-04-19T01:38:22.060699Z"},"trusted":true},"outputs":[],"source":["import requests\n","\n","# Download the stopwords file\n","url = \"https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\"\n","response = requests.get(url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Extract stopwords from the content\n","    stopwords = response.text.split(\",\")\n","else:\n","    print(\"Failed to download stopwords file.\")\n","\n","# Stopwords removal function\n","def remove_stopwords(text):\n","    if isinstance(text, str):\n","        # Split the text into tokens using whitespace as delimiter\n","        tokens = text.split()\n","        # Remove stopwords from the tokens\n","        filtered_tokens = [word for word in tokens if word.lower() not in stopwords]\n","        # Join the filtered tokens back into a string\n","        filtered_text = ' '.join(filtered_tokens)\n","        return filtered_text\n","    else:\n","        return text\n","\n","\n","\n","\n","\n","# Applying the stopwords removal function to the 'text' column in the train_data DataFrame\n","train_data['text_without_stopwords'] = train_data['text'].apply(remove_stopwords)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:38:33.731014Z","iopub.status.busy":"2024-04-19T01:38:33.730511Z","iopub.status.idle":"2024-04-19T01:38:33.753932Z","shell.execute_reply":"2024-04-19T01:38:33.753033Z","shell.execute_reply.started":"2024-04-19T01:38:33.730981Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>text_lower</th>\n","      <th>tokens</th>\n","      <th>text_without_stopwords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id have responded if i were going</td>\n","      <td>neutral</td>\n","      <td>i`d have responded, if i were going</td>\n","      <td>[id, have, responded, if, i, were, going]</td>\n","      <td>responded</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sooo sad i will miss you here in san diego</td>\n","      <td>negative</td>\n","      <td>sooo sad i will miss you here in san diego!!!</td>\n","      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n","      <td>sooo sad san diego</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>my boss is bullying me</td>\n","      <td>negative</td>\n","      <td>my boss is bullying me...</td>\n","      <td>[my, boss, is, bullying, me]</td>\n","      <td>boss bullying</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>what interview leave me alone</td>\n","      <td>negative</td>\n","      <td>what interview! leave me alone</td>\n","      <td>[what, interview, leave, me, alone]</td>\n","      <td>interview leave</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sons of  why couldnt they put them on the rel...</td>\n","      <td>negative</td>\n","      <td>sons of ****, why couldn`t they put them on t...</td>\n","      <td>[sons, of, why, couldnt, they, put, them, on, ...</td>\n","      <td>sons releases bought</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31010</th>\n","      <td>its at 3 am im very tired but i cant sleep  bu...</td>\n","      <td>negative</td>\n","      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n","      <td>[its, at, 3, am, im, very, tired, but, i, cant...</td>\n","      <td>3 tired sleep</td>\n","    </tr>\n","    <tr>\n","      <th>31011</th>\n","      <td>all alone in this old house again  thanks for ...</td>\n","      <td>positive</td>\n","      <td>all alone in this old house again.  thanks for...</td>\n","      <td>[all, alone, in, this, old, house, again, than...</td>\n","      <td>house net alive kicking invented net wanna kis...</td>\n","    </tr>\n","    <tr>\n","      <th>31012</th>\n","      <td>i know what you mean my little dog is sinking...</td>\n","      <td>negative</td>\n","      <td>i know what you mean. my little dog is sinkin...</td>\n","      <td>[i, know, what, you, mean, my, little, dog, is...</td>\n","      <td>dog sinking depression someplace tropical</td>\n","    </tr>\n","    <tr>\n","      <th>31013</th>\n","      <td>sutra what is your next youtube video gonna be...</td>\n","      <td>positive</td>\n","      <td>_sutra what is your next youtube video gonna b...</td>\n","      <td>[sutra, what, is, your, next, youtube, video, ...</td>\n","      <td>sutra youtube video gonna love videos</td>\n","    </tr>\n","    <tr>\n","      <th>31014</th>\n","      <td>omgssh  ang cute ng bby</td>\n","      <td>positive</td>\n","      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n","      <td>[omgssh, ang, cute, ng, bby]</td>\n","      <td>omgssh ang cute bby</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31014 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                    text sentiment  \\\n","0                      id have responded if i were going   neutral   \n","1             sooo sad i will miss you here in san diego  negative   \n","2                                 my boss is bullying me  negative   \n","3                          what interview leave me alone  negative   \n","4       sons of  why couldnt they put them on the rel...  negative   \n","...                                                  ...       ...   \n","31010  its at 3 am im very tired but i cant sleep  bu...  negative   \n","31011  all alone in this old house again  thanks for ...  positive   \n","31012   i know what you mean my little dog is sinking...  negative   \n","31013  sutra what is your next youtube video gonna be...  positive   \n","31014                            omgssh  ang cute ng bby  positive   \n","\n","                                              text_lower  \\\n","0                    i`d have responded, if i were going   \n","1          sooo sad i will miss you here in san diego!!!   \n","2                              my boss is bullying me...   \n","3                         what interview! leave me alone   \n","4       sons of ****, why couldn`t they put them on t...   \n","...                                                  ...   \n","31010  its at 3 am, im very tired but i can`t sleep  ...   \n","31011  all alone in this old house again.  thanks for...   \n","31012   i know what you mean. my little dog is sinkin...   \n","31013  _sutra what is your next youtube video gonna b...   \n","31014   http://twitpic.com/4woj2 - omgssh  ang cute n...   \n","\n","                                                  tokens  \\\n","0              [id, have, responded, if, i, were, going]   \n","1      [sooo, sad, i, will, miss, you, here, in, san,...   \n","2                           [my, boss, is, bullying, me]   \n","3                    [what, interview, leave, me, alone]   \n","4      [sons, of, why, couldnt, they, put, them, on, ...   \n","...                                                  ...   \n","31010  [its, at, 3, am, im, very, tired, but, i, cant...   \n","31011  [all, alone, in, this, old, house, again, than...   \n","31012  [i, know, what, you, mean, my, little, dog, is...   \n","31013  [sutra, what, is, your, next, youtube, video, ...   \n","31014                       [omgssh, ang, cute, ng, bby]   \n","\n","                                  text_without_stopwords  \n","0                                              responded  \n","1                                     sooo sad san diego  \n","2                                          boss bullying  \n","3                                        interview leave  \n","4                                   sons releases bought  \n","...                                                  ...  \n","31010                                      3 tired sleep  \n","31011  house net alive kicking invented net wanna kis...  \n","31012          dog sinking depression someplace tropical  \n","31013              sutra youtube video gonna love videos  \n","31014                                omgssh ang cute bby  \n","\n","[31014 rows x 5 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:48:34.702502Z","iopub.status.busy":"2024-04-19T01:48:34.700655Z","iopub.status.idle":"2024-04-19T01:48:34.722585Z","shell.execute_reply":"2024-04-19T01:48:34.721428Z","shell.execute_reply.started":"2024-04-19T01:48:34.702432Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Encode the sentiment labels\n","label_encoder = LabelEncoder()\n","train_data['sentiment_encoded'] = label_encoder.fit_transform(train_data['sentiment'])\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T01:51:08.747752Z","iopub.status.busy":"2024-04-19T01:51:08.747284Z","iopub.status.idle":"2024-04-19T01:51:54.927204Z","shell.execute_reply":"2024-04-19T01:51:54.925741Z","shell.execute_reply.started":"2024-04-19T01:51:08.747720Z"},"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","\n","# Step 1: Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(train_data['text_without_stopwords'],train_data['sentiment_encoded'], test_size=0.2, random_state=42)\n","\n","# Step 2: Vectorize the text data using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# # Step 3: Train the SVM classifier\n","# svm_classifier = SVC(kernel='linear')\n","# svm_classifier.fit(X_train_tfidf, y_train)\n","\n","# # Step 4: Predict sentiment on the test set\n","# y_pred = svm_classifier.predict(X_test_tfidf)\n","\n","# # Step 5: Evaluate the model\n","# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","# print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"ename":"ValueError","evalue":"in user code:\n\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 10) vs (None, 1)).\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 6: Train the ANN on the training data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Step 7: Evaluate the performance of the ANN on the testing data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_tfidf, y_test)\n","File \u001b[1;32mc:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekg3mw7eb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\91798\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 10) vs (None, 1)).\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","# Step 4: Define the architecture of the ANN\n","model = Sequential()\n","model.add(Dense(128, input_shape=(X_train_tfidf.shape[1],), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Step 5: Compile the ANN\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Step 6: Train the ANN on the training data\n","history = model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_split=0.1)\n","\n","# Step 7: Evaluate the performance of the ANN on the testing data\n","loss, accuracy = model.evaluate(X_test_tfidf, y_test)\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{},"source":["**For improving accuracy**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T03:54:55.997729Z","iopub.status.busy":"2024-04-19T03:54:55.997130Z","iopub.status.idle":"2024-04-19T04:54:48.186060Z","shell.execute_reply":"2024-04-19T04:54:48.183906Z","shell.execute_reply.started":"2024-04-19T03:54:55.997691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters: {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n","Accuracy: 0.6595195872964694\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.54      0.62      1749\n","           1       0.58      0.77      0.67      2502\n","           2       0.76      0.62      0.69      1952\n","\n","    accuracy                           0.66      6203\n","   macro avg       0.69      0.64      0.66      6203\n","weighted avg       0.68      0.66      0.66      6203\n","\n"]}],"source":["# Re-split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(train_data['text_without_stopwords'], train_data['sentiment_encoded'], test_size=0.2, random_state=42)\n","\n","# Vectorize the text data using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Tune hyperparameters of the SVM classifier\n","from sklearn.model_selection import GridSearchCV\n","\n","# Define the parameter grid\n","param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'sigmoid']}\n","\n","# Instantiate the GridSearchCV object\n","grid_search = GridSearchCV(SVC(), param_grid, cv=3, n_jobs=-1)\n","\n","# Fit the GridSearchCV object to the training data\n","grid_search.fit(X_train_tfidf, y_train)\n","\n","# Get the best parameters\n","best_params = grid_search.best_params_\n","print(\"Best Parameters:\", best_params)\n","\n","# Use the best parameters to train the SVM classifier\n","best_svm_classifier = SVC(**best_params)\n","best_svm_classifier.fit(X_train_tfidf, y_train)\n","\n","# Predict sentiment on the test set\n","y_pred = best_svm_classifier.predict(X_test_tfidf)\n","\n","# Evaluate the model\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":989445,"sourceId":1808590,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
